function testLQRN;

% Declare global variables 
global N A b Q R my0 S0 gamma  

% Set problem size 
N = 1;  % Set the dimensionality of the states 

% Start state distribution
my0 = 0.3*ones(N,1);
S0  = 0.0001*eye(N);

% Transition matrix A and coupling vector b
A = eye(N);
b = ones(N,1); 

% Reward matrix Q and scalar R 
Q = 1;
R = 0.1;

% Discounted reward formulation
gamma = 0.95;

initializeLQRProblem;

% Select a Gauss policy
policy = initGaussPolicy;

% Select initial policy
policy.theta.k     = -0.5; % Set controller gains...
policy.theta.sigma =  0.2; % Set controller variance...

% Obtain Data
data  = obtainData(policy, 100, 1);

Xs = data(1).x(:,1:100);

figure(1); clf; hold on;
plot(Xs,ideal(policy,data),'b*');
plot(Xs,idealApproximation(policy,data),'ro');

% plot(Xs,localMonteCarlo(data),'gd');
plot(Xs,locallyWeightedDynamicProgramming(data),'gd');
%plot(Xs,locallyWeightedDynamicProgramming(data)-idealApproximation(policy,data),'c.')
plot(Xs(1:99),locallyWeightedResidualGradients(data),'k>');

function V = ideal(policy,data)
    for i=1:100
       x = data(1).x(:,i);
       V(1,i) = VFnc(policy,x);            
    end;

function Vapp = approx(Xs,Ys)
    sigma = 0.2;
    for i=1:100
       x = Xs(i);

       P(i,:) = [x 1];
       w(i,1) = exp(-0.5*((x-0.4)/sigma)^2)/sqrt(2*pi*sigma^2);
       w(i,2) = exp(-0.5*((x+0.4)/sigma)^2)/sqrt(2*pi*sigma^2);
       V(i,1) = Ys(i);            
    end;
    
    V1 = (P*inv(P'*diag(w(:,1))*P)*P'*diag(w(:,1))*V);
    V2 = (P*inv(P'*diag(w(:,2))*P)*P'*diag(w(:,2))*V);
    Vapp = (w(:,1) .* V1 + w(:,2) .* V2) ./ (w(:,1)+w(:,2));    
    
function Vapp = approx2(Xs,Ys)
    global gamma
    sigma = 0.2;
    for i=1:99
       x = Xs(i);
       P(i,:) = [(x-gamma*Xs(i+1)) (1-gamma)];
       w(i,1) = exp(-0.5*((x-0.4)/sigma)^2)/sqrt(2*pi*sigma^2);
       w(i,2) = exp(-0.5*((x+0.4)/sigma)^2)/sqrt(2*pi*sigma^2);
       V(i,1) = Ys(i);            
    end;
    
    V1 = (P*inv(P'*diag(w(:,1))*P)*P'*diag(w(:,1))*V);
    V2 = (P*inv(P'*diag(w(:,2))*P)*P'*diag(w(:,2))*V);
    Vapp = (w(:,1) .* V1 + w(:,2) .* V2) ./ (w(:,1)+w(:,2));    

    
    
function Vapp = idealApproximation(policy,data)
    for i=1:100
       Xs(i,1) = data(1).x(:,i);
       Ys(i,1) = VFnc(policy,Xs(i));            
    end;
    
    Vapp = approx(Xs,Ys);  
    
        
function Vapp = localMonteCarlo(data)
    for i=1:100
       Xs(i,1) = data(1).x(:,i);
       Ys(i,1) = sum(data(1).r(i:100));           
    end;
    
    Vapp = approx(Xs,Ys);  

function Vapp = locallyWeightedDynamicProgramming(data)
   global gamma

   Vapp      = data(1).r;
   
   for t=1:100
       Vapp(101) = 0;

       for i=1:100
          Xs(i,1) = data(1).x(:,i);
          Ys(i,1) = data(1).r(i) + gamma*Vapp(i+1);           
       end;
    
       Vapp = approx(Xs,Ys);  
   end;
   
function Vapp = locallyWeightedResidualGradients(data)
   global gamma
   
   for i=1:100
       Xs(i,1) = data(1).x(:,i);
       Ys(i,1) = data(1).r(i);           
   end;
   
   Vapp = approx2(Xs,Ys);  
   